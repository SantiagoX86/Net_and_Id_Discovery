# Test Strategy

**Enterprise Security Discovery Orchestration Framework**

* **Organization:** WAUIG Bank
* **Project Name:** Enterprise Security Discovery Orchestration Framework
* **Document Version:** 1.0
* **Document Date:** 2025-11-22
* **Author:** Sean Santiago
* **Phase:** SDLC Phase 5 – Test

---

## 1. Purpose

This document defines the **test strategy** for the Enterprise Security Discovery Orchestration Framework. The goal of testing is to validate that the framework:

* Accurately reflects externally observable exposure
* Behaves deterministically under controlled environmental changes
* Produces auditable, repeatable discovery output
* Maintains strict separation between discovery logic and reporting logic

Testing emphasizes **behavioral correctness and exposure accuracy**, not exploitation, persistence, or offensive techniques.

---

## 2. Testing Scope

### 2.1 In Scope

* Core orchestration framework (M1)
* Network Discovery domain (M2)
* Output & Reporting layer (M3)
* Agentless discovery from a low-privilege external vantage point
* Environmental posture changes (firewall rules, network profile, service exposure)

### 2.2 Out of Scope

* Exploitation or payload execution
* Credentialed discovery
* Persistence or lateral movement
* Performance and load testing
* Production deployment validation

---

## 3. Test Objectives

The primary objectives of this test phase are to verify that:

1. Discovery results accurately reflect **real network exposure**
2. Firewall and configuration changes result in **observable output deltas**
3. Blocked or filtered services are **not falsely reported as exposed**
4. Reporting output correctly summarizes findings without altering evidence
5. The framework remains stable as new discovery domains are introduced

---

## 4. Test Environment

### 4.1 Lab Topology

* **Source Host:** Kali Linux VM
* **Target Host:** Windows Sandbox VM
* **Network Mode:** VMware Host-Only Networking
* **Discovery Methods:** ICMP echo, TCP connect
* **Credentials:** None (unauthenticated)

### 4.2 Environmental Controls

The following configuration elements are intentionally modified during testing:

* Windows Defender Firewall rules (per-port allow/deny)
* Windows network profile (Public vs Private)
* Service state (listening vs not listening)

These changes are used to validate **before-and-after discovery behavior**.

---

## 5. Test Types

### 5.1 Functional Testing

Validates that each discovery component behaves as designed.

Examples:

* Host presence detection via ICMP
* TCP port discovery for enterprise-relevant services
* Correct categorization of findings (e.g., `open_port`, `host_presence`)

### 5.2 Configuration Change Validation

Validates that discovery output changes **only when exposure actually changes**.

Examples:

* Firewall rule added → port becomes externally visible
* Firewall rule removed → port no longer visible
* ICMP blocked → host still observable via TCP behavior

### 5.3 Regression Testing (Manual)

Ensures previously validated behavior remains intact after changes.

Examples:

* Adding reporting logic does not alter discovery output
* Refactoring output formatting does not drop findings
* Network Discovery remains stable after lab reconfiguration

---

## 6. Test Execution Approach

Testing is performed using:

* A developer harness (`testing_testing.py`)
* Manual validation via `nc`, `ping`, and Windows PowerShell
* Controlled environment configuration changes
* Side-by-side comparison of raw findings and generated reports

This hybrid approach mirrors common enterprise validation workflows for internal security tooling.

---

## 7. Pass / Fail Criteria

### 7.1 Pass Criteria

* Open services are detected only when externally reachable
* Filtered or blocked services are not falsely reported as exposed
* Discovery output remains consistent across repeated runs
* Reporting accurately reflects underlying findings

### 7.2 Fail Criteria

* False positives for non-exposed services
* Missed detection of truly open services
* Discovery behavior altered by reporting logic
* Non-deterministic results without environmental change

---

## 8. Risks and Assumptions

### 8.1 Assumptions

* The tester has full control over the lab environment
* Network conditions are stable during test execution
* Discovery activities are intentionally non-intrusive

### 8.2 Risks

* OS-specific networking behavior may affect probe classification
* Firewall profile changes may occur automatically
* Timeout behavior may vary across platforms

These risks are documented and accepted for this phase.

---

## 9. Exit Criteria

Phase 5 (Test) is considered complete when:

* Core framework, Network Discovery, and Reporting are validated
* Known limitations are documented
* Discovery behavior is proven repeatable
* The project is ready for iterative refinement or additional discovery domains

---

## 10. Next Planned Steps

Upon completion of this test phase:

* Iterative improvements may be applied to reporting clarity
* Additional discovery domains (Identity, Host Configuration, etc.) may be implemented
* Formal test cases and test result summaries may be expanded
